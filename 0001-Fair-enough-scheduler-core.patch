From a8b71cbeb446e5668267e7b7df96d40d982f2c4f Mon Sep 17 00:00:00 2001
From: Nicolas Manichon <nmanichon@gmail.com>
Date: Fri, 22 Jan 2021 15:12:14 +0100
Subject: [PATCH 1/2] Fair enough scheduler core

---
 include/asm-generic/vmlinux.lds.h |   1 +
 include/linux/sched.h             |   6 +
 include/uapi/linux/sched.h        |   1 +
 init/init_task.c                  |   5 +-
 kernel/kthread.c                  |   2 +-
 kernel/sched/Makefile             |   2 +-
 kernel/sched/core.c               |  20 ++-
 kernel/sched/fair_enuf.c          | 289 ++++++++++++++++++++++++++++++
 kernel/sched/sched.h              |  18 +-
 9 files changed, 334 insertions(+), 10 deletions(-)
 create mode 100644 kernel/sched/fair_enuf.c

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index b2b3d81b1535..444b57f26a85 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -132,6 +132,7 @@
 	*(__fair_sched_class)			\
 	*(__rt_sched_class)			\
 	*(__dl_sched_class)			\
+	*(__fe_sched_class)                     \
 	*(__stop_sched_class)			\
 	__end_sched_classes = .;
 
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 76cd21fa5501..4bfd07805040 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -581,6 +581,11 @@ struct sched_dl_entity {
 #endif
 };
 
+struct sched_fe_entity {
+	struct list_head list;
+	int time;
+};
+
 #ifdef CONFIG_UCLAMP_TASK
 /* Number of utilization clamp buckets (shorter alias) */
 #define UCLAMP_BUCKETS CONFIG_UCLAMP_BUCKETS_COUNT
@@ -695,6 +700,7 @@ struct task_struct {
 	struct task_group		*sched_task_group;
 #endif
 	struct sched_dl_entity		dl;
+	struct sched_fe_entity		fe;
 
 #ifdef CONFIG_UCLAMP_TASK
 	/*
diff --git a/include/uapi/linux/sched.h b/include/uapi/linux/sched.h
index 3bac0a8ceab2..39826a28c0b7 100644
--- a/include/uapi/linux/sched.h
+++ b/include/uapi/linux/sched.h
@@ -118,6 +118,7 @@ struct clone_args {
 /* SCHED_ISO: reserved but not implemented yet */
 #define SCHED_IDLE		5
 #define SCHED_DEADLINE		6
+#define SCHED_FAIR_ENUF        42
 
 /* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */
 #define SCHED_RESET_ON_FORK     0x40000000
diff --git a/init/init_task.c b/init/init_task.c
index a56f0abb63e9..c36b1397d5b6 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -78,7 +78,7 @@ struct task_struct init_task
 	.prio		= MAX_PRIO - 20,
 	.static_prio	= MAX_PRIO - 20,
 	.normal_prio	= MAX_PRIO - 20,
-	.policy		= SCHED_NORMAL,
+	.policy		= SCHED_FAIR_ENUF,
 	.cpus_ptr	= &init_task.cpus_mask,
 	.cpus_mask	= CPU_MASK_ALL,
 	.nr_cpus_allowed= NR_CPUS,
@@ -94,6 +94,9 @@ struct task_struct init_task
 		.run_list	= LIST_HEAD_INIT(init_task.rt.run_list),
 		.time_slice	= RR_TIMESLICE,
 	},
+	.fe             = {
+		.list = LIST_HEAD_INIT(init_task.fe.list),
+	},
 	.tasks		= LIST_HEAD_INIT(init_task.tasks),
 #ifdef CONFIG_SMP
 	.pushable_tasks	= PLIST_NODE_INIT(init_task.pushable_tasks, MAX_PRIO),
diff --git a/kernel/kthread.c b/kernel/kthread.c
index 933a625621b8..69fe35f38e6a 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -383,7 +383,7 @@ struct task_struct *__kthread_create_on_node(int (*threadfn)(void *data),
 		 * root may have changed our (kthreadd's) priority or CPU mask.
 		 * The kernel thread should not inherit these properties.
 		 */
-		sched_setscheduler_nocheck(task, SCHED_NORMAL, &param);
+		sched_setscheduler_nocheck(task, SCHED_FAIR_ENUF, &param);
 		set_cpus_allowed_ptr(task,
 				     housekeeping_cpumask(HK_FLAG_KTHREAD));
 	}
diff --git a/kernel/sched/Makefile b/kernel/sched/Makefile
index 5fc9c9b70862..3918fca13c6f 100644
--- a/kernel/sched/Makefile
+++ b/kernel/sched/Makefile
@@ -23,7 +23,7 @@ CFLAGS_core.o := $(PROFILING) -fno-omit-frame-pointer
 endif
 
 obj-y += core.o loadavg.o clock.o cputime.o
-obj-y += idle.o fair.o rt.o deadline.o
+obj-y += idle.o fair.o rt.o deadline.o fair_enuf.o
 obj-y += wait.o wait_bit.o swait.o completion.o
 
 obj-$(CONFIG_SMP) += cpupri.o cpudeadline.o topology.o stop_task.o pelt.o
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index e7e453492cff..7852288548e9 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -3093,6 +3093,9 @@ static void __sched_fork(unsigned long clone_flags, struct task_struct *p)
 	p->rt.on_rq		= 0;
 	p->rt.on_list		= 0;
 
+	INIT_LIST_HEAD(&p->fe.list);
+	p->fe.time		= 100;
+
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 	INIT_HLIST_HEAD(&p->preempt_notifiers);
 #endif
@@ -3245,7 +3248,7 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 	 */
 	if (unlikely(p->sched_reset_on_fork)) {
 		if (task_has_dl_policy(p) || task_has_rt_policy(p)) {
-			p->policy = SCHED_NORMAL;
+			p->policy = SCHED_FAIR_ENUF;
 			p->static_prio = NICE_TO_PRIO(0);
 			p->rt_priority = 0;
 		} else if (PRIO_TO_NICE(p->static_prio) < 0)
@@ -3261,7 +3264,9 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 		p->sched_reset_on_fork = 0;
 	}
 
-	if (dl_prio(p->prio))
+	if (p->policy == SCHED_FAIR_ENUF)
+		p->sched_class = &fe_sched_class;
+	else if (dl_prio(p->prio))
 		return -EAGAIN;
 	else if (rt_prio(p->prio))
 		p->sched_class = &rt_sched_class;
@@ -4339,10 +4344,10 @@ pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
 	 * higher scheduling class, because otherwise those loose the
 	 * opportunity to pull in more work from other CPUs.
 	 */
-	if (likely(prev->sched_class <= &fair_sched_class &&
-		   rq->nr_running == rq->cfs.h_nr_running)) {
+	if (likely(prev->sched_class <= &fe_sched_class &&
+		   rq->nr_running == rq->fe.nr_running)) {
 
-		p = pick_next_task_fair(rq, prev, rf);
+		p = pick_next_task_fe(rq);
 		if (unlikely(p == RETRY_TASK))
 			goto restart;
 
@@ -5186,7 +5191,9 @@ static void __setscheduler(struct rq *rq, struct task_struct *p,
 	if (keep_boost)
 		p->prio = rt_effective_prio(p, p->prio);
 
-	if (dl_prio(p->prio))
+	if (p->policy == SCHED_FAIR_ENUF)
+		p->sched_class = &fe_sched_class;
+	else if (dl_prio(p->prio))
 		p->sched_class = &dl_sched_class;
 	else if (rt_prio(p->prio))
 		p->sched_class = &rt_sched_class;
@@ -7144,6 +7151,7 @@ void __init sched_init(void)
 		init_cfs_rq(&rq->cfs);
 		init_rt_rq(&rq->rt);
 		init_dl_rq(&rq->dl);
+		init_fe_rq(&rq->fe);
 #ifdef CONFIG_FAIR_GROUP_SCHED
 		INIT_LIST_HEAD(&rq->leaf_cfs_rq_list);
 		rq->tmp_alone_branch = &rq->leaf_cfs_rq_list;
diff --git a/kernel/sched/fair_enuf.c b/kernel/sched/fair_enuf.c
new file mode 100644
index 000000000000..631e6eab5a34
--- /dev/null
+++ b/kernel/sched/fair_enuf.c
@@ -0,0 +1,289 @@
+#include "sched.h"
+
+static struct task_struct *list_to_task_struct(struct list_head *l)
+{
+	return container_of(list_entry(l, struct sched_fe_entity, list),
+			    struct task_struct, fe);
+}
+
+#define FE_DEBUG 0
+
+#if FE_DEBUG
+#define DBG(...)                                                               \
+	do {                                                                   \
+		pr_info("Currently running: %d\n", rq->curr->pid);             \
+		pr_info(__VA_ARGS__);                                          \
+	} while (0)
+
+static void dump_rq(struct list_head *h)
+{
+	struct list_head *p;
+	struct task_struct *t;
+
+	list_for_each (p, h) {
+		t = list_to_task_struct(p);
+		pr_info("-> %d\n", t->pid);
+	}
+}
+
+static size_t list_length(struct list_head *h)
+{
+	size_t ret = 0;
+	struct list_head *p;
+
+	list_for_each (p, h) {
+		ret++;
+	}
+
+	return ret;
+}
+
+#else
+#define DBG(...)
+#endif
+
+static int task_in_rq(struct rq *rq, struct task_struct *task)
+{
+	struct list_head *curr;
+
+	list_for_each (curr, &rq->fe.entity_list) {
+		if (task == list_to_task_struct(curr))
+			return 1;
+	}
+
+	return 0;
+}
+
+/*
+ * Called by sched_init, _very_ early during kernel startup.
+ * Initialize the per-CPU runqueue.
+ */
+void init_fe_rq(struct fe_rq *rq)
+{
+	rq->nr_running = 0;
+	INIT_LIST_HEAD(&rq->entity_list);
+}
+
+/*
+ * Called when a task is ready to run.
+ * - When a task is created (wake_up_new_task -> activate_task)
+ * - When a task that was waiting, or blocked is now ready
+ * Add the task to the runqueue and increment the count of tasks that could run.
+ * Mark the task as ready (on_rq = 1)
+ *
+ * Quirks:
+ * - Can be called with p == rq->curr.
+ */
+static void enq_task_fe(struct rq *rq, struct task_struct *p,
+			int flags /* unused */)
+{
+	struct sched_fe_entity *fe_it = &p->fe;
+
+	DBG("enqueue(%d, %d) (prio: %d)\n", p->pid, flags, p->prio);
+
+	BUG_ON(task_in_rq(rq, p));
+	if (rq->curr != p)
+		list_add_tail(&fe_it->list, &rq->fe.entity_list);
+
+	p->on_rq = 1;
+	rq->fe.nr_running += 1;
+	add_nr_running(rq, 1);
+}
+
+/*
+ * Called when a task that was running/ready is now blocked or waiting.
+ * Remove the task from the runqueue and decrement the count of tasks that could
+ * run.
+ * Mark the task as blocked (on_rq = 0)
+ */
+static void deq_task_fe(struct rq *rq, struct task_struct *task, int flags)
+{
+	DBG("deq_task_fe(%d, %d)\n", task->pid, flags);
+	BUG_ON(task != rq->curr && !task_in_rq(rq, task));
+
+	list_del_init(&task->fe.list);
+
+	rq->fe.nr_running -= 1;
+	sub_nr_running(rq, 1);
+	task->on_rq = 0;
+}
+
+static void yield_task_fe(struct rq *rq /* unused */)
+{
+	BUG();
+}
+
+/*
+ * The core of the scheduler.
+ * Returns the task that should replace the task that is currently running.
+ * If we have nothing to schedule, return NULL.
+ * Can return the currently running task if it should keep running.
+ * Reset the tick counter for the task.
+ *
+ * Quirks:
+ * - The kernel might not always call put_prev_task after pick_next_task, so
+ * call put_prev_task.
+ */
+struct task_struct *pick_next_task_fe(struct rq *rq)
+{
+	struct fe_rq *frq = &rq->fe;
+	struct task_struct *current_task = rq->curr;
+	struct task_struct *next_task;
+
+	DBG("pick_next_task\n");
+#if FE_DEBUG
+	dump_rq(&frq->entity_list);
+#endif
+
+	/*
+	 * If we have nothing in the runqueue, but the current task is not
+	 * blocked, we can keep running the current task.
+	 * If the current task is blocked, then we have nothing to schedule,
+	 * and the current task must stop, so just return NULL.
+	 */
+	if (list_empty(&frq->entity_list)) {
+		if (current_task->on_rq) {
+			DBG("nothing on the rq, current can continue\n");
+			return current_task;
+		}
+
+		DBG("Nothing to schedule\n");
+		return NULL;
+	}
+
+	/*
+	 * We run the process at the head of the queue (FIFO).
+	 */
+	next_task = list_to_task_struct(frq->entity_list.next);
+	list_del_init(frq->entity_list.next);
+
+	DBG("pick_next_task_fe() current: %d -> next: %d\n", current_task->pid,
+	    next_task->pid);
+
+	next_task->fe.time = 100;
+
+	/*
+	 * put_prev_task will add the current task to the runqueue if it was
+	 * preempted.
+	 */
+	put_prev_task(rq, current_task);
+
+	return next_task;
+}
+
+/*
+ * Called when a task that was running is kicked out.
+ * This can be called by the scheduler or by pick_next_task.
+ *
+ * If the task is not blocked (on_rq = 1), then add it to the runqueue.
+ * This is necessary because we do not keep the currently running task in the
+ * runqueue.
+ */
+static void put_prev_task_fe(struct rq *rq, struct task_struct *p)
+{
+	DBG("put_prev_task_fe(%d) rq length: %zu\n", p->pid,
+	    list_length(&rq->fe.entity_list));
+
+	BUG_ON(task_in_rq(rq, p));
+
+	if (p->on_rq) {
+		DBG("The task has been preempted, re add to the runqueue\n");
+		list_add_tail(&p->fe.list, &rq->fe.entity_list);
+	}
+}
+
+/*
+ * The scheduler might call this to replace the currenlty running task with p,
+ * without letting us choose (!= pick_next_task).
+ *
+ * We just remove the task from the runqueue.
+ */
+static void set_next_task_fe(struct rq *rq /* unused */, struct task_struct *p,
+			     bool first)
+{
+	DBG("set_next_task_fe(%d, %d)\n", p->pid, first);
+
+	list_del_init(&p->fe.list);
+}
+
+/*
+ * A tick that allows us to compute the time a task has been running.
+ * This makes it possible for us to ask the kernel to reschedule the current
+ * process if it has been running long enough.
+ */
+static void task_tick_fe(struct rq *rq, struct task_struct *p, int queued)
+{
+	BUG_ON(rq->curr != p);
+
+	DBG("task_tick_fe(%d): queued = %d | Ticks left: %d (%d)\n", p->pid,
+	    queued, p->fe.time, test_tsk_need_resched(p));
+
+	if (--p->fe.time <= 0) {
+		DBG("Asking for resched of %d\n", p->pid);
+		resched_curr(rq);
+	}
+}
+
+/* Keep in mind that as task_struct->prio decreases, priority increases */
+
+/*
+ * Called when a process is ready to run.
+ * If the task has higher priority than the currently running task, ask
+ * the kernel to reschedule when possible.
+ */
+static void check_preempt_curr_fe(struct rq *rq, struct task_struct *p,
+				  int flags /* unused */)
+{
+	DBG("check_preempt_curr(%d, %d) (prio: %d): current %d (prio: %d)\n",
+	    p->pid, flags, p->prio, rq->curr->pid, rq->curr->prio);
+	BUG_ON(!p->on_rq);
+
+	if (p == rq->curr)
+		return;
+
+	if (p->prio < rq->curr->prio) {
+		DBG("  => Reschedule");
+		resched_curr(rq);
+	}
+}
+
+/*
+ * Called when the priority of a task changes.
+ *
+ * This triggers a rescheduling if:
+ * - The task that changed priority is ready
+ * - The priority of the task increased
+ */
+static void prio_changed_fe(struct rq *rq, struct task_struct *p, int oldprio)
+{
+	DBG("prio_changed_fe(%d) prio was %d, now %d\n", p->pid, oldprio,
+	    p->prio);
+
+	if (p->on_rq && p->prio < oldprio) {
+		DBG("  => Reschedule");
+		resched_curr(rq);
+	}
+}
+
+static void update_curr_fe(struct rq *rq /* unused */)
+{
+	BUG();
+}
+
+const struct sched_class fe_sched_class __section("__fe_sched_class") = {
+	.enqueue_task = enq_task_fe,
+	.dequeue_task = deq_task_fe,
+	.yield_task = yield_task_fe,
+
+	.check_preempt_curr = check_preempt_curr_fe,
+
+	.pick_next_task = pick_next_task_fe,
+	.put_prev_task = put_prev_task_fe,
+	.set_next_task = set_next_task_fe,
+
+	.task_tick = task_tick_fe,
+
+	.prio_changed = prio_changed_fe,
+
+	.update_curr = update_curr_fe,
+};
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index df80bfcea92e..e12b8207cb07 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -176,10 +176,16 @@ static inline int dl_policy(int policy)
 {
 	return policy == SCHED_DEADLINE;
 }
+
+static inline bool fe_policy(int policy)
+{
+	return policy == SCHED_FAIR_ENUF;
+}
+
 static inline bool valid_policy(int policy)
 {
 	return idle_policy(policy) || fair_policy(policy) ||
-		rt_policy(policy) || dl_policy(policy);
+		rt_policy(policy) || dl_policy(policy) || fe_policy(policy);
 }
 
 static inline int task_has_idle_policy(struct task_struct *p)
@@ -713,6 +719,11 @@ struct dl_rq {
 	u64			bw_ratio;
 };
 
+struct fe_rq {
+	size_t nr_running;
+	struct list_head entity_list;
+};
+
 #ifdef CONFIG_FAIR_GROUP_SCHED
 /* An entity is a task if it doesn't "own" a runqueue */
 #define entity_is_task(se)	(!se->my_q)
@@ -931,6 +942,7 @@ struct rq {
 	struct cfs_rq		cfs;
 	struct rt_rq		rt;
 	struct dl_rq		dl;
+	struct fe_rq		fe;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* list of leaf cfs_rq on this CPU: */
@@ -1865,6 +1877,7 @@ extern const struct sched_class dl_sched_class;
 extern const struct sched_class rt_sched_class;
 extern const struct sched_class fair_sched_class;
 extern const struct sched_class idle_sched_class;
+extern const struct sched_class fe_sched_class;
 
 static inline bool sched_stop_runnable(struct rq *rq)
 {
@@ -1889,6 +1902,8 @@ static inline bool sched_fair_runnable(struct rq *rq)
 extern struct task_struct *pick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf);
 extern struct task_struct *pick_next_task_idle(struct rq *rq);
 
+extern struct task_struct *pick_next_task_fe(struct rq *rq);
+
 #ifdef CONFIG_SMP
 
 extern void update_group_capacity(struct sched_domain *sd, int cpu);
@@ -2281,6 +2296,7 @@ print_numa_stats(struct seq_file *m, int node, unsigned long tsf,
 extern void init_cfs_rq(struct cfs_rq *cfs_rq);
 extern void init_rt_rq(struct rt_rq *rt_rq);
 extern void init_dl_rq(struct dl_rq *dl_rq);
+extern void init_fe_rq(struct fe_rq* rq);
 
 extern void cfs_bandwidth_usage_inc(void);
 extern void cfs_bandwidth_usage_dec(void);
-- 
2.30.0

